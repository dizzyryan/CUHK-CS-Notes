\chapter{Bayesian Statistic}

\section{Statistic v.s. Probability}

Statistics focuses on real-life applications where the underlying distribution is often unknown. To address this, we use \textbf{statistical inference} to analyze observed data and estimate the unknown distribution. Rather than finding the exact distribution, we approximate it using models such as parametric (e.g., normal, exponential) or non-parametric approaches. Once a suitable model is chosen, probability laws help us make predictions and draw conclusions, though these approximations involve assumptions and uncertainties.  

Statistics and probability are closely related but serve different purposes. \textbf{Probability} predicts outcomes based on known models, while \textbf{statistics} analyzes past data to infer unknown distributions. Probability starts with assumptions and calculates outcomes, whereas statistics starts with data to estimate models.  

Despite their differences, the two fields are interdependent. Probability provides the foundation for statistical inference, while statistics helps validate probability models using real-world data. In short, probability predicts based on models, while statistics builds models from data.  

Now, let's move on to our first topic in statistics: 


\section{Bayesian Statistics}
In the probability course, we learned Bayes' Rule (ENGG2760: Theorem 3.2.1), which helps us calculate conditional probabilities and, at times, update our beliefs based on new evidence.

