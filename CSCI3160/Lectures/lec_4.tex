\section{Fast Fourier Transform}
For degree-\(d\) polynomials, 
\[
  A(x) = \sum_{i = 0}^d a_i x^i,\quad B(x) = \sum_{i = 0}^d b_i x^i,
\]
their product \(C(x) = A(x)B(x) = \sum_{k = 0}^{2d} c_k x^k\) with 
\[
  c_k = \sum_{i = 0}^k a_i b_{k - i}
\]
where \(a_i, b_i = 0\) if \(i > d\). 

To do such multiplication it takes \(\Theta(d^2)\). To speed up, we again use divide and conquer. 

\subsection{Point-wise Multiplication}
First, we take a look at the representation of a polynomial. Consider two representations of \(A(x)\):  

1. Coefficients \(a_0, a_1, \dots, a_d\)

2. Values at \(d + 1\) distinct points (point–value pairs):
\[
  (x_0, A(x_0)), (x_1, A(x_1)), \dots, (x_d, A(x_d))
\]

\begin{remark}
  A degree-\(d\) polynomial is uniquely determined by its values at any \(d + 1\) distinct points. 
\end{remark}

If we use point–value representation, given \(C(x) = A(x)B(x)\), for any point \(z\) we have \(C(z) = A(z)B(z)\). Thus, it takes linear time to compute all the values of \(C(x)\), i.e. \((2d + 1) = \mathcal{O}(d)\) multiplications. 

\subsection{Root of Unity}
Before obtaining \(C(x)\), we need to evaluate \(A(x)\) and \(B(x)\). Evaluating a degree-\(n\) polynomial at one point costs at least \(\mathcal{O}(n)\) time; since there are \(n\) points in total, this takes \(\Theta(n^2)\). However, consider the following. 

A polynomial can be decomposed as (assume \(n\) is even):
\[
  \begin{aligned}
    A(x) &= a_0 + a_1 x + a_2 x^2 + \dots + a_n x^n \\
    &= (a_0 + a_2 x^2 + \dots + a_n x^n) + x(a_1 + a_3 x^2 + \dots + a_{n-1} x^{n-2}) \\
    &= A_{\text{even}}(x^2) + x \cdot A_{\text{odd}}(x^2)
  \end{aligned}
\]
where
\[
  A_{\text{even}}(x) = a_0 + a_2 x + \dots + a_n x^{\frac{n}{2}}, \quad A_{\text{odd}}(x) = a_1 + a_3 x + \dots + a_{n-1} x^{\frac{n}{2} - 1}.
\]
Then, if we consider paired points \(\pm x_i\), we have 
\[
  A(x_i) = A_{\text{even}}(x_i^2) + x_i A_{\text{odd}}(x_i^2),\quad A(-x_i) = A_{\text{even}}(x_i^2) - x_i A_{\text{odd}}(x_i^2).
\]
Then the evaluation on points is reduced to the evaluation of \(\{x_0^2, \dots, x_{\frac{n}{2} - 1}^2\}\) plus a linear-time combination. 

However, this reduction cannot utilize recursion; therefore, we choose another set of points using the concept of complex numbers. 

\begin{theorem}[Fundamental Theorem of Algebra]
  Every polynomial of degree \(n \geq 1\) with complex coefficients has exactly \(n\) roots in \(\mathbb{C}\), counted with multiplicity. Equivalently, for 
  \[
    p(z) = a_n z^n + a_{n-1} z^{n-1} + \cdots + a_1 z + a_0,\quad (a_n \neq 0),
  \]
  there exist \(\zeta_1, \dots, \zeta_n \in \mathbb{C}\) such that 
  \[
    p(z) = a_n \prod_{k = 1}^{n} (z - \zeta_k),
  \]
  where each root appears according to its multiplicity.
  
  \begin{explanation}
    Consider our problem: we have a polynomial \(C(x)\) that can be determined by \(n = 2d + 1\) points. According to the theorem, every polynomial of degree \(2d\) has exactly \(2d\) roots in \(\mathbb{C}\). Hence, we can use these complex roots to find a convenient set of points that make evaluation easier.
  \end{explanation}
\end{theorem}

\begin{definition}
  An \(n\)-th root of unity, where \(n\) is a positive integer, is a complex number \(z\) satisfying 
  \[
    z^n = 1.
  \]
  There are \(n\) distinct \(n\)-th roots of unity, given by
  \[
    z_k = \cos \left(\dfrac{2 k \pi}{n}\right) + i \sin \left(\dfrac{2 k \pi}{n}\right), \quad \forall k \in \{0, 1, \dots, n - 1\}.
  \]
\end{definition}

Let
\[
  \omega_n = \cos \left(\dfrac{2\pi}{n}\right) + i \sin \left(\dfrac{2\pi}{n}\right),
\]
then we have
\[
  \begin{aligned}
    \omega_n^0 &= \cos \left(0\right) + i \sin \left(0\right) = 1, \\
    \omega_n^1 &= \cos \left(\dfrac{2\pi}{n}\right) + i \sin \left(\dfrac{2\pi}{n}\right), \\
    \omega_n^2 &= \cos \left(\dfrac{4\pi}{n}\right) + i \sin \left(\dfrac{4\pi}{n}\right), \\
    &\ \vdots \\
    \omega_n^{n-1} &= \cos \left(\dfrac{2 (n-1) \pi}{n}\right) + i \sin \left(\dfrac{2 (n-1) \pi}{n}\right).
  \end{aligned}
\]
Thus, we can write the \(n\)-th roots of unity as 
\[
  \omega_n^0, \omega_n^1, \omega_n^2, \dots, \omega_n^{n-2}, \omega_n^{n-1}. 
\]
By Euler's formula, 
\[
  e^{i\theta} = \cos(\theta) + i \sin(\theta),
\]
we have
\[
  \omega_n^k = \cos \left(\dfrac{2k\pi}{n}\right) + i \sin \left(\dfrac{2k\pi}{n}\right) = e^{i \frac{2k\pi}{n}}.
\]
Moreover, we note some important facts about the roots of unity:
\begin{proposition}
  For all \(n \in \mathbb{N}\), there are exactly \(n\) distinct \(n\)-th roots of unity, i.e.,
  \[
    U_n = \{\omega_n^0, \omega_n^1, \omega_n^2, \dots, \omega_n^{n-1}\}.
  \]
\end{proposition}
\begin{proposition}
  For all even \(n \in \mathbb{N}\), it holds that 
  \[
    \omega_n^{k + \frac{n}{2}} = -\omega_n^k, \quad \forall k \in \{0, 1, \dots, \tfrac{n}{2} - 1\}.
  \]
\end{proposition}
\begin{proposition}
  3. For all even \(n \in \mathbb{N}\), squaring each element of \(U_n\) gives the set \(U_{\frac{n}{2}}\), i.e., the set of \(\frac{n}{2}\)-th roots of unity. More explicitly, 
  \[
    \omega_n^{2k} = \omega_{\frac{n}{2}}^{k \bmod (\frac{n}{2})}.
  \]

  \begin{proof}
    Let \(\omega_n = e^{\frac{2\pi i}{n}}\) be a primitive \(n\)-th root of unity. Consider the square of \(\omega_n^k\):
    \[
      (\omega_n^k)^2 = (e^{\frac{2k\pi i}{n}})^2 = e^{\frac{2(2k)\pi i}{n}} = e^{\frac{2k\pi i}{\frac{n}{2}}} = \omega_{\frac{n}{2}}^{k}.
    \]
    Thus, for each \(k = 0 , 1, \dots, n-1\), squaring \(\omega_n^k\) gives \(\omega_{n/2}^{k}\).  
  \end{proof}
  \begin{remark}~

    - Since \(k\) ranges over \(0, \dots, n - 1\), some values of \(\omega_{n/2}^k\) repeat (because \(k\) modulo \(\frac{n}{2}\)).  

    - Therefore, the set of all \((\omega_n^k)^2\) is exactly
    \[
      \{\omega_{n/2}^0, \omega_{n/2}^1, \dots, \omega_{n/2}^{n/2-1}\} = U_{n/2},
    \]
    the set of \(\frac{n}{2}\)-th roots of unity.
    \end{remark}
\end{proposition}


\subsection{Fast Fourier Transform}
Now we can look at the details of the Fast Fourier Transform (FFT). 

Given a polynomial \(A(x)\) of degree at most \(n - 1\), where \(n = 2^m\) for some \(m \geq 0\). 

If \(n = 1\), then \(\deg A(x) \leq 0\), and \(A(x) = a_0\). 

If \(n \geq 2\), 

1. Split \(A(x)\) into its even and odd parts. 

2. Make two recursive FFT calls of size \(\frac{n}{2}\) to evaluate both \(A_{\text{even}}(y)\) and \(A_{\text{odd}}(y)\) on all the points in 
\[
  U_{\frac{n}{2}} = \{\omega_{\frac{n}{2}}^0, \omega_{\frac{n}{2}}^1, \dots, \omega_{\frac{n}{2}}^{\frac{n}{2} - 1}\}.
\]

3. For all \(k \in \{0, 1, \dots, n - 1\}\), compute 
\[
  A(\omega_n^k) = A_{\text{even}}(\omega_n^{2k}) + \omega_n^{k} A_{\text{odd}}(\omega_n^{2k}),
\]
using the values of \(A_{\text{even}}(y)\) and \(A_{\text{odd}}(y)\) at 
\(\omega_n^{2k} = \omega_{\frac{n}{2}}^{k \bmod (\frac{n}{2})}\). 

4. Output the values \((A(\omega_n^k))_{k=0}^{n-1}\). 

Then we have the time complexity
\[
  f(n) =
  \begin{dcases}
    \mathcal{O}(1), & \text{if } n = 1;\\[4pt]
    2f\!\left(\tfrac{n}{2}\right) + \mathcal{O}(n), & \text{if } n \geq 2.
  \end{dcases}
\]

\begin{explanation}
Suppose we want to evaluate a polynomial at \(n = 8\) points.  

\begin{enumerate}
  \item Split into even and odd parts:
  \[
    A(x) = A_{\text{even}}(x^2) + x A_{\text{odd}}(x^2)
  \]
  This reduces the 8-point evaluation to two 4-point evaluations at \(x^2\).
  
  \item Recursively split each 4-point evaluation into even/odd again, reducing to 2-point evaluations at \(x^4\).
  
  \item Split again until reaching 1-point evaluations (just the coefficients themselves).
  
  \item Combine results on the way back using
  \[
    A(\omega_n^k) = A_{\text{even}}(\omega_{n/2}^k) + \omega_n^k \, A_{\text{odd}}(\omega_{n/2}^k),
  \]
  which takes \(O(n)\) time per level.
\end{enumerate}

\textbf{Key idea:} At each level, the recursive results are reused for multiple points; only the multiplication by \(\omega_n^k\) differs. This divide-and-conquer gives total complexity \(O(n \log n)\).
\end{explanation}

The last step is to turn the point-value pair representation back to the coefficient representation, which is called interpolation. We can accomplish the interpolation in \(\mathcal{O}(n \log n)\) time by using the inverse FFT. 

\subsection{Analysis}
We can summarize all the operations here. 

First, we select the smallest \(n \geq 2d + 1\) where \(n = 2^m\), \(m \geq 0\). 

Then we do the evaluation using FFT for both \(A(\omega_n^k)\) and \(B(\omega_n^k)\), which takes \(\mathcal{O}(n \log n)\). 

We then do the point-wise multiplication, i.e. \(C(\omega_n^k) = A(\omega_n^k)B(\omega_n^k)\), which takes \(\mathcal{O}(n)\). 

Last, we do the interpolation using inverse FFT, taking \(\mathcal{O}(n \log n)\) time. 

Thus, we can finish a degree-\(n\) polynomial multiplication in time \(\mathcal{O}(n \log n)\).
