\chapter{Divide and Conquer}

We take a look at divide and conquer, which guarantees strong performance. When dividing, we utilize recursion to reduce the problem into sub-problems, and the conquer part tackles the original problem. 

\section{Sorting}
We consider \href{https://www.ryanc.wtf/files/CSCI2100.pdf#page=41}{merge sort} in this example.

\textbf{Problem Statement}

Given an array \(A\) of \(n\) distinct integers, produce another array where the same integers are arranged in ascending order.

\textbf{Divide}

Let \(A_1\) be the array containing the first \(\lceil \frac{n}{2} \rceil\) elements of \(A\), and \(A_2\) be the array containing the remaining elements of \(A\). We sort \(A_1\) and \(A_2\) recursively. 

\textbf{Conquer}

Merge the two sorted arrays in ascending order, which can be done in \(\mathcal{O}(n)\) time. 

\textbf{Analysis}

Let \(f(n)\) denote the worst-case cost of the algorithm on an array of size \(n\). Then, 
\[
  f(n) \leq 2f\!\left(\left\lceil \dfrac{n}{2} \right\rceil\right) + \mathcal{O}(n),
\]
which gives\footnote{See Master Theorem \ref{mastertheorem}.}
\[
  f(n) = \mathcal{O}(n \log n).
\]

\section{Counting Inversions}
\textbf{Problem Statement}

Given an array \(A\) of \(n\) distinct integers, count the number of inversions, where an inversion is a pair \((i, j)\) such that \(1 \leq i < j \leq n\) and \(A[i] > A[j]\).

\textbf{Divide}

Let \(A_1\) be the array containing the first \(\lceil \frac{n}{2} \rceil\) elements of \(A\), and \(A_2\) be the array containing the remaining elements of \(A\). We solve the problem recursively on \(A_1\) and \(A_2\). 

\textbf{Conquer}

It remains to count the number of \emph{crossing inversions} \((i, j)\) where \(i \in A_1\) and \(j \in A_2\). Using merge sort, we can sort \(A_1\) in \(\mathcal{O}(n \log n)\) time. For each element \(e \in A_2\), we count how many crossing inversions \(e\) produces using binary search. In total, there are \(\frac{n}{2}\) binary searches performed, each taking \(\mathcal{O}(\log n)\) time, giving \(\mathcal{O}(n \log n)\) time for this step. 

\textbf{Analysis}

A simple comparison of each pair takes 
\[
  (n - 1) + (n - 2) + \cdots + 1 = \mathcal{O}(n^2).
\]
Let \(f(n)\) denote the worst-case cost of the algorithm on an array of size \(n\). Then, 
\[
  f(n) \leq 2f\!\left(\left\lceil \dfrac{n}{2} \right\rceil\right) + \mathcal{O}(n \log n),
\]
which gives
\[
  f(n) = \mathcal{O}(n \log^2 n).
\]

\section{Dominance Counting}
\textbf{Problem Statement}

Denote \(\mathbb{Z}\) as the set of integers. Given a point \(p\) in \(\mathbb{Z}^2\), denoted by \(p[1], p[2]\) its \(x\)- and \(y\)-coordinate, given two distinct points \(p\) and \(q\), we say that \(q\) dominates \(p\) if \(p[1] \leq q[1]\) and \(p[2] \leq q[2]\). 

Let \(P\) be a set of \(n\) points in \(\mathbb{Z}^2\) with distinct \(x\)-coordinates. Find for each point \(p \in P\) the number of points in \(P\) that are dominated by \(p\). 

Assume that, without loss of generality, the points are given in ascending order in the \(x\)-coordinates, i.e. 
\[
  p_1[1] < p_2[1] < \cdots < p_n[1].
\]

\textbf{Divide}

Let \(l\) be the vertical line such that \(P\) has \(\lceil \frac{n}{2} \rceil\) points on each side of the line, where \(P_1\) is the set of points of \(P\) on the left of \(l\), and \(P_2\) is the set of points on the right of \(l\). We solve the problem recursively on \(P_1\) and \(P_2\). 

\textbf{Conquer}

It remains to count for each point \(p_2 \in P_2\) how many points in \(P_1\) it dominates. We sort \(P_1\) by its \(y\)-coordinate. Then for each point \(p_2 \in P_2\), we obtain the number of points in \(P_1\) dominated by \(p_2\) using binary search. 

\textbf{Analysis}

Let \(f(n)\) denote the worst-case cost of the algorithm on \(n\) points. In total, there are \(\frac{n}{2}\) binary searches performed, each taking \(\mathcal{O}(\log n)\) time, giving \(\mathcal{O}(n \log n)\) time for this step. Then, 
\[
  f(n) \leq 2f\!\left(\left\lceil \dfrac{n}{2} \right\rceil\right) + \mathcal{O}(n \log n),
\]
which gives
\[
  f(n) = \mathcal{O}(n \log^2 n).
\]

\section{Matrix Multiplication (Strassen's Algorithm)}
Given two \(n \times n\) matrices \(A\) and \(B\), compute their product \(AB\). Assume for simplicity that \(n\) is a power of 2. We can divide each of \(A\) and \(B\) into 4 sub-matrices of order \(\frac{n}{2}\). 

Suppose we want to compute 
\[
  \begin{bmatrix}
    A_{11} & A_{12} \\
    A_{21} & A_{22} \\
  \end{bmatrix}\begin{bmatrix}
    B_{11} & B_{12}  \\
    B_{21} & B_{22}  \\
  \end{bmatrix}.
\] 
We need to perform 8 order-\(\frac{n}{2}\) matrix multiplications in the most trivial case, i.e. 
\[
  \begin{bmatrix}
    A_{11} & A_{12} \\
    A_{21} & A_{22} \\
  \end{bmatrix}\begin{bmatrix}
    B_{11} & B_{12}  \\
    B_{21} & B_{22}  \\
  \end{bmatrix} = \begin{bmatrix}
    A_{11} B_{11} + A_{12} B_{21} & A_{11} B_{12}  + A_{12} B_{22}  \\
    A_{21} B_{11} + A_{22} B_{21} & A_{21} B_{12}  + A_{22} B_{22} 
  \end{bmatrix}.
\]
In the trivial case, we need \(\mathcal{O} (n^3)\) time. 

In a non-trivial case, we have
\[
  \begin{bmatrix}
    A_{11} & A_{12} \\
    A_{21} & A_{22} \\
  \end{bmatrix}\begin{bmatrix}
    B_{11} & B_{12}  \\
    B_{21} & B_{22}  \\
  \end{bmatrix} = \begin{bmatrix}
    p_5 + p_4 - p_2 + p_6 & p_1 + p_2 \\
    p_3 + p_4 & p_1 + p_5 - p_3 - p_7 \\
  \end{bmatrix} 
\]
where
\[
  \begin{aligned}
    p_1 &= A_{11}(B_{12}  - B_{22} ) \\
    p_2 &= (A_{11} + A_{12})B_{22}  \\
    p_3 &= (A_{21} + A_{22})B_{11} \\
    p_4 &= A_{22}(B_{21} - B_{11}) \\
    p_5 &= (A_{11} + A_{22})(B_{11} + B_{22} ) \\
    p_6 &= (A_{12} - A_{22})(B_{21} + B_{22} ) \\
    p_7 &= (A_{11} - A_{21})(B_{11} + B_{12} ) \\
  \end{aligned}
\]
If \(f(n)\) is the worst-case time of computing the product of two order-\(n\) matrices, then each of \(p_i, 1 \leq i \leq 7\) can be computed in \(f(\frac{n}{2}) + \mathcal{O}(n^2)\) time, where \(f(\frac{n}{2})\) is exactly one multiplication between two order-\(\frac{n}{2}\) matrices, and \(\mathcal{O} (n^2)\) is the cost of matrix additions and subtractions.

Therefore, 
\[
  f(n) \leq 7f\left(\dfrac{n}{2}\right) + \mathcal{O}(n^2)
\]
can be solved to 
\[
  f(n) = \mathcal{O}(n^{\log_2 7}) = \mathcal{O} (n^{2.81}). 
\]

\begin{remark}
  For \(n\) being a power of 2, we can recursively split the matrix into sub-matrices, and it still takes \(\mathcal{O}(n^{2.81})\). This also works for any \(n\) since we can pad with zeros if \(n\) is not a power of two. Given that the logic here does not depend on \(n\), the running time remains \(\mathcal{O}(n^{2.81})\).
\end{remark}