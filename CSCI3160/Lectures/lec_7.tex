\section{Huffman Codes}
Consider an alphabet \(\Sigma\). An \textbf{encoding} is a function that maps each letter in \(\Sigma\) to a binary string, where such a string is called a \textbf{code-word}. 

Given that not all symbols occur with equal frequency, we can assign shorter code-words to more frequent symbols in order to reduce the \textbf{average number of bits per letter}. However, we must enforce a constraint that no letter's code-word is a prefix of another letter's code-word. An encoding satisfying this constraint is called a \textbf{prefix code}. 

For each letter \(\sigma \in \Sigma\), let \(\textit{freq}(\sigma)\) denote the frequency of \(\sigma\), and let \(\textit{len}(\sigma)\) denote the number of bits in the code-word of \(\sigma\). Then, given an encoding, its average length is 
\[
  \sum_{\sigma \in \Sigma} \textit{freq}(\sigma) \cdot \textit{len}(\sigma).
\]

\begin{problem}[The Prefix Code Problem]
  Given an alphabet \(\Sigma\) and the frequency of each letter in it, find a prefix code for \(\Sigma\) with the smallest possible average length. 
\end{problem}

We can utilize a binary tree to represent prefix codes. 

\begin{definition}[Code Tree]
  A \textbf{code tree} on \(\Sigma\) is a binary tree \(T\) such that every leaf node corresponds to a unique letter in \(\Sigma\), and every letter in \(\Sigma\) corresponds to a unique leaf node in \(T\). For every internal node, its left edge is labeled \(0\) and its right edge is labeled \(1\).
\end{definition}

Then we can generate the prefix code for a letter \(\sigma \in \Sigma\) from the tree \(T\) by concatenating the bit labels of the edges on the path from the root to \(\sigma\).

\begin{lemma}
  Every prefix code can be generated by a code tree.
\end{lemma}

Since the code word length corresponds to the level of the node in \(T\), to solve the problem we can simply find a tree \(T\) that minimizes the average height.

\textbf{Greedy Algorithm}

Let \(n = \vert \Sigma \vert\). Create a set \(S\) of \(n\) stand-alone leaf nodes, each corresponding to a distinct letter in \(\Sigma\). Then repeat the following steps until \(\vert S \vert = 1\):

1. Remove from \(S\) two nodes \(u_1\) and \(u_2\) with the smallest frequencies.

2. Create a new node \(v\) with \(u_1\) and \(u_2\) as its children, and set the frequency of \(v\) to be the sum of the frequencies of \(u_1\) and \(u_2\).

3. Add \(v\) back into \(S\).

Here the prefix code derived from the tree is called \textbf{Huffman code}. 

\begin{lemma}
  In an optimal code tree, every internal node of \(T\) must have two children. 
\end{lemma}
\begin{proof}
  Assume for contradiction, \(T\) is optimal but contains an internal node \(v\) with only one child. 

  Let the single child-subtree of \(v\) contain the leaf set \(L\) and 
  \[
    W = \sum_{x \in L} p_x
  \]
  be the total weight of those leaves. Since all \(p_x > 0\), we have \(W > 0\). 

  Now form a new tree \(T^{\prime}\) by bypassing \(v\), i.e., remove \(v\) and attach its only child in \(v\)'s place. Then all leaves in \(L\) move up one level, so each such leaf's depth decreases by exactly 1. 

  Let \(C(T)\) denote the expected cost of tree \(T\), 
  \[
    C(T) = \sum_{\text{leaves }x} p_x \cdot d_T(x) 
  \]
  Then the new cost will be 
  \[
    C^{\prime}(T) = C(T) - \sum_{x \in L} p_x = C(T) - W
  \]
  Because \(W > 0\), we have \(C^{\prime}(T) < C(T)\), leading to contradiction. 
\end{proof}

\begin{lemma}
  Let \(\sigma_1, \sigma_2\) be two letters in \(\Sigma\) with the lowest frequencies. There exists an optimal code tree where \(\sigma_1\) and \(\sigma_2\) have the same parent. 
\end{lemma}
\begin{proof}
  Without loss of generality, assume \(\textit{freq}(\sigma_1) \leq \textit{freq}(\sigma_2)\). Let \(T\) be any optimal code tree. Let \(p\) be an arbitrary internal node with the largest level in \(T\). By Lemma 3.3.2, \(p\) must have two leaves. Let \(x\) and \(y\) be letters corresponding to those leaves such that \(\textit{freq}(x) \leq \textit{freq}(y)\). Swap \(\sigma_1\) with \(x\) and \(\sigma_2\) with \(y\), which gives a new code tree \(T^{\prime}\). Note that both \(\sigma_1\) and \(\sigma_2\) are children of \(p\) in \(T^{\prime}\). 
\end{proof}

\begin{theorem}
  Huffman code produces an optimal prefix code. 
\end{theorem}
\begin{proof}
  Consider \(\sigma\) with a size of \(n\). 

  For \(n = 2\), Huffman algorithm will encode one letter with 0 and the other letter with 1, which is optimal. 

  Assume the theorem is correct for \(n = k - 1\) where \(k \geq 3\). 

  Consider the following: 

  \(\Sigma\): an alphabet of size \(k\), where \(\sigma_1\) and \(\sigma_2\) are two letters with the lowest frequencies. 
  
  \(T\): an optimal code tree on \(\Sigma\), where leaves \(\sigma_1\) and \(\sigma_2\) have the same parent \(p\). 

  \(T_{\textit{huff}}\): output of Huffman's algorithm on \(\Sigma\). 
  
  And for \(n = k - 1\), we have 
  
  \(\Sigma^{\prime}\): an alphabet constructed from \(\Sigma\) by removing \(\sigma_1\) and \(\sigma_2\) and adding a letter \(\sigma^{\star}\) with frequency \(\textit{freq}(\sigma_1) + \textit{freq}(\sigma_2)\). 
  
  \(T^{\prime}\): the tree obtained by removing leaves \(\sigma_1\) and \(\sigma_2\) from \(T\). 

  \(T_{\textit{huff}}^{\prime}\): output of Huffman's algorithm on \(\Sigma^{\prime}\). 

  Then we have 
  \[
    \text{average height of } T_{\textit{huff}} = \text{average height of } T_{\textit{huff}}^{\prime} + \textit{freq}(\sigma_1) + \textit{freq}(\sigma_2), 
  \] 
  \[
    \text{average height of } T = \text{average height of } T^{\prime} + \textit{freq}(\sigma_1) + \textit{freq}(\sigma_2), 
  \]
  \[
    \text{average height of } T_{\textit{huff}} \leq \text{average height of } T^{\prime}, 
  \]
  which give
  \[
    \text{average height of } T_{\textit{huff}} \leq \text{average height of } T
  \]
\end{proof}

\textbf{Analysis}
We can implement this algorithm in \(\mathcal{O}(n \log n)\) time. 