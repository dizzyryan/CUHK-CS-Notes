\chapter{Introduction}

Computer science is a subject where we first define a computation model, which is a simple yet accurate abstraction of a computing machine, and then we gradually build up a theory based on this model. 

Thus, the first thing to do is to come up with a computation model. For the study of algorithms, we need to consider the following criteria: 

1. \textbf{Mechanically Implementable:} it can run real algorithms.

2. \textbf{Sufficiently General:} it can capture all the natural steps of problem-solving.

3. \textbf{Sensitive Enough:} it can distinguish differences in resource usage (time and space).

We choose the \textbf{Random Access Machine (RAM)} as our model.

\section{The RAM Model}
The RAM model has a \textbf{memory} and a \textbf{CPU}. 

Memory is defined as an infinite sequence of cells, each containing the same number of \(w\) bits, with addresses 1, 2, and so on. The CPU contains a fixed number of registers, each of which has \(w\) bits. A word is a sequence of \(w\) bits, where \(w\) is called the word length. In other words, each memory cell and CPU register stores a word. 

\subsection{Atomic Operations}
We say that there are a few atomic operations that the CPU can perform. 

1. \textbf{Register (Re-)Initialization} 

We can set a register to a fixed value or to the content of another register. 

2. \textbf{Arithmetic} 

This operation takes two integers stored in two registers and performs basic arithmetic calculations. 

3. \textbf{Comparison / Branching}

This operation takes two integers stored in two registers, compares them, and determines the result of the comparison.

4. \textbf{Memory Access}

This operation takes a memory address that is currently stored in a register, then performs either reading (to register) or writing (to memory). 

5. \textbf{Randomness} 

\texttt{RANDOM(x, y)} returns an integer chosen \textbf{uniformly} at random in \([x, y]\), where \(x \leq y\). The resulting random integer is then placed in a register.

An execution is defined as a sequence of atomic operations, where the cost (running time) of an execution is the length of such a sequence, i.e., the number of atomic operations it performs.

\subsection{Algorithms}
We first take a look at some terminologies. 

An input refers to the initial state of the registers and the memory before an execution starts.

An algorithm is a description that, given an input, can be utilized to \textbf{unambiguously} produce a sequence of atomic operations — namely, the execution of the algorithm. In other words, it should always be clear what the next atomic operation should be, given the outcomes of all the previous atomic operations.

The cost of an algorithm on an input is the length of its execution on that input (i.e., the number of atomic operations required).

The space of an algorithm on an input is the largest memory address accessed by the algorithm’s execution on that input.

We define an algorithm as \textbf{deterministic} if it never invokes the atomic operation \texttt{RANDOM}; otherwise, the algorithm is \textbf{randomized}. 

\subsection{Expected Running Time}
A deterministic algorithm has a fixed cost for the same input, whereas for a randomized algorithm, the cost is a random variable, since for each input the cost might change every time the algorithm is executed. 

\begin{algorithm}[H]
	\DontPrintSemicolon{}
	\caption{Example}
	\BlankLine
	\While(){\(r \neq 1\)}{
		\(r = \) \texttt{RANDOM(0, 1)}
	}
	\Return{\(r = 1\)}\;
\end{algorithm}

Here, we cannot know how many times line 2 will be executed, as each execution can produce a new sequence of atomic operations.

Thus, we use the \textbf{expected cost} to evaluate the cost of a randomized algorithm. 

\begin{definition}
  Let \(X\) be a random variable that represents the cost of an algorithm on a given input. The expected cost of the algorithm on that input is the expectation of \(X\). 
\end{definition}

We use the expected running time, rather than other metrics, for the following reasons: 

1. Ease of computation

2. Linearity of expectation: it allows us to break the analysis into smaller parts

3. Concentration bounds

\begin{theorem}[Markov's Inequality]
  Suppose that a random variable \(X \geq 0\) only takes non-negative values. Then, for every \(t > 0\),
  \[
      \mathbb{P}(X \geq t) \leq \dfrac{\mathbb{E}[X]}{t}.
  \]
\end{theorem}

The theorem here shows that if the average value of \(X\) is small, then it is less likely that \(X\) is large. This provides an upper bound on the probability that a non-negative random variable is much larger than its expectation. 

Let \(T\) be the random variable representing the running time of a randomized algorithm, and suppose \(T \geq 0\) and \(\mathbb{E}[T] = \mu\). Then, for any \(c > 1\),
\[
    \mathbb{P}(T \geq c \mu) \leq \dfrac{1}{c}.
\]

\begin{eg}[Las Vegas Algorithm]
  A Las Vegas algorithm always gives the correct result but has a random running time.

  Suppose \(\mathbb{E}[T] = \mu\), and we run this algorithm with a timeout of \(c \mu\). Then we have 
  \[
    \mathbb{P}[\text{timeout}] \leq \dfrac{1}{c}.
  \]

  This shows that if we run the algorithm for \(c\) times its expected running time, then the probability that it fails will be less than or equal to \(\frac{1}{c}\). 

  For example, if we run for \(3\mu\) steps, then it will fail with probability \(\leq \frac{1}{3}\). Therefore, if we repeat it 3 times, we will succeed with probability \(1 - \left(\frac{1}{3}\right)^3 = 0.963\).
\end{eg}