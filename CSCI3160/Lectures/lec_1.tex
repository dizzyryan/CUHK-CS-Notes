\chapter{Introduction}

Computer science is a subject where we first define a computation model, which is a simple yet accurate abstraction of a computing machine, and then we gradually build up a theory based on this model. 

Thus, the first thing to do is to come up with a computation model. For the study of algorithms, we need to consider the following criteria: 

1. \textbf{Mechanically Implementable:} it can run real algorithms.

2. \textbf{Sufficiently General:} it can capture all the natural steps of problem-solving.

3. \textbf{Sensitive Enough:} it can distinguish differences in resource usage (time and space).

We choose the \textbf{Random Access Machine (RAM)} as our model.

\section{The RAM Model}
The RAM model has a \textbf{memory} and a \textbf{CPU}. 

Memory is defined as an infinite sequence of cells, each containing the same number of \(w\) bits, with addresses 1, 2, and so on. The CPU contains a fixed number of registers, each of which has \(w\) bits. A word is a sequence of \(w\) bits, where \(w\) is called the word length. In other words, each memory cell and CPU register stores a word. 

\subsection{Atomic Operations}
We say that there are a few atomic operations that the CPU can perform. 

1. \textbf{Register (Re-)Initialization} 

We can set a register to a fixed value or to the content of another register. 

2. \textbf{Arithmetic} 

This operation takes two integers stored in two registers and performs basic arithmetic calculations. 

3. \textbf{Comparison / Branching}

This operation takes two integers stored in two registers, compares them, and determines the result of the comparison.

4. \textbf{Memory Access}

This operation takes a memory address that is currently stored in a register, then performs either reading (to register) or writing (to memory). 

5. \textbf{Randomness} 

\texttt{RANDOM(x, y)} returns an integer chosen \textbf{uniformly} at random in \([x, y]\), where \(x \leq y\). The resulting random integer is then placed in a register.

An execution is defined as a sequence of atomic operations, where the cost (running time) of an execution is the length of such a sequence, i.e., the number of atomic operations it performs.

\subsection{Algorithms}
We first take a look at some terminologies. 

An input refers to the initial state of the registers and the memory before an execution starts.

An algorithm is a description that, given an input, can be utilized to \textbf{unambiguously} produce a sequence of atomic operations — namely, the execution of the algorithm. In other words, it should always be clear what the next atomic operation should be, given the outcomes of all the previous atomic operations.

The cost of an algorithm on an input is the length of its execution on that input (i.e., the number of atomic operations required).

The space of an algorithm on an input is the largest memory address accessed by the algorithm’s execution on that input.

We define an algorithm as \textbf{deterministic} if it never invokes the atomic operation \texttt{RANDOM}; otherwise, the algorithm is \textbf{randomized}. 

\subsection{Expected Running Time}
A deterministic algorithm has a fixed cost for the same input, whereas for a randomized algorithm, the cost is a random variable, since for each input the cost might change every time the algorithm is executed. 

\begin{algorithm}[H]
	\DontPrintSemicolon{}
	\caption{Find a One}
	\BlankLine
	\While(){\(r \neq 1\)}{
		\(r = \) \texttt{RANDOM(0, 1)}
	}
	\Return{\(r = 1\)}\;
\end{algorithm}

Here, we cannot know how many times line 2 will be executed, as each execution can produce a new sequence of atomic operations.

Thus, we use the \textbf{expected cost} to evaluate the cost of a randomized algorithm. 

\begin{definition}
  Let \(X\) be a random variable that represents the cost of an algorithm on a given input. The expected cost of the algorithm on that input is the expectation of \(X\). 
\end{definition}

We use the expected running time, rather than other metrics, for the following reasons: 

1. Ease of computation

2. Linearity of expectation: it allows us to break the analysis into smaller parts

3. Concentration bounds

\begin{theorem}[Markov's Inequality]
  Suppose that a random variable \(X \geq 0\) only takes non-negative values. Then, for every \(t > 0\),
  \[
      \mathbb{P}(X \geq t) \leq \dfrac{\mathbb{E}[X]}{t}.
  \]
\end{theorem}

The theorem here shows that if the average value of \(X\) is small, then it is less likely that \(X\) is large. This provides an upper bound on the probability that a non-negative random variable is much larger than its expectation. 

Let \(T\) be the random variable representing the running time of a randomized algorithm, and suppose \(T \geq 0\) and \(\mathbb{E}[T] = \mu\). Then, for any \(c > 1\),
\[
    \mathbb{P}(T \geq c \mu) \leq \dfrac{1}{c}.
\]

\begin{eg}[Las Vegas Algorithm]
  A Las Vegas algorithm always gives the correct result but has a random running time.

  Suppose \(\mathbb{E}[T] = \mu\), and we run this algorithm with a timeout of \(c \mu\). Then we have 
  \[
    \mathbb{P}[\text{timeout}] \leq \dfrac{1}{c}.
  \]

  This shows that if we run the algorithm for \(c\) times its expected running time, then the probability that it fails will be less than or equal to \(\frac{1}{c}\). 

  For example, if we run for \(3\mu\) steps, then it will fail with probability \(\leq \frac{1}{3}\). Therefore, if we repeat it 3 times, we will succeed with probability \(1 - \left(\frac{1}{3}\right)^3 = 0.963\).
\end{eg}

\section{Efficiency of the Worst Input}
By looking at the cost of an algorithm on the worst input, we are able to measure the speed of an algorithm.

\begin{definition}
  Define \(\mathcal{I}_n\), where \(n\) is an integer, to be the set of all inputs to a problem that have the same problem size \(n\). Given an input \(I \in \mathcal{I}_n\), the cost \(X_{\mathcal{A}}(I)\) of an algorithm \(\mathcal{A}\) is the length of its execution on \(I\). 

  The worst-case cost of \(\mathcal{A}\) under the problem size \(n\) is the maximum \(X_{\mathcal{A}}(I)\) over all \(I \in \mathcal{I}_n\). 

  The worst expected cost of \(\mathcal{A}\) under the problem size \(n\) is the maximum \(\mathbb{E}[X_{\mathcal{A}}(I)]\) over all \(I \in \mathcal{I}_n\). 
\end{definition}

\begin{eg}[Dictionary Search]
  In memory, a set \(S\) of \(n\) integers has been arranged in ascending order in the memory cells from address 1 to \(n\). The value of \(n\) has been placed in Register 1 of the CPU. Another integer \(v\) has been placed in Register 2, where \(n\) is the problem size, and \(\mathcal{I}_n\) is the set of all possible \((S, v)\). 

  We want to determine whether \(v\) exists in \(S\). 

  The worst-case cost of the \href{https://www.ryanc.wtf/files/CSCI2100.pdf#page=16}{binary search algorithm} is \(\mathcal{O}(\log n)\). This means that on any input in \(\mathcal{I}_n\), the maximum number \(f(n)\) of atomic operations performed by the algorithm grows no faster than \(\log_2 n\). 
\end{eg}

\begin{eg}[Randomized Algorithm]
  Consider the following randomized algorithm: 
  
  \begin{algorithm}[H]
	\DontPrintSemicolon{}
  \KwData{\(A\) is an array of size \(n\) that contains at least one 0}
	\caption{Find a Zero}
	\BlankLine
	\While(){\(A[r] \neq 0\)}{
		\(r = \) \texttt{RANDOM(1, n)}
	}
	\Return{\(r\)}\;
  \end{algorithm}

  The expected cost depends on the input array. For example, if all numbers in \(A\) are 0, then the algorithm finishes in \(\mathcal{O}(1)\). If it has only one 0, the algorithm will finish in \(\mathcal{O}(n)\) because each \(A[r]\) has a \(\frac{1}{n}\) probability of being 0, and we need to repeat \(n\) times in expectation to find the 0. 
  
  Thus, we have worst-case cost \(= \infty\) and worst expected cost \(= \mathcal{O}(n)\). 
\end{eg}

\begin{eg}[Find a Zero]
  Let \(A\) be an array of \(n\) integers, among which half are 0. Design an algorithm to report an arbitrary position of \(A\) that contains a 0. 

  \begin{algorithm}[H]
	\DontPrintSemicolon{}
	\caption{Find a Zero}
	\BlankLine
	\While(){\(A[r] \neq 0\)}{
		\(r = \) \texttt{RANDOM(1, n)}
	}
	\Return{\(r\)}\;
  \end{algorithm}

  The algorithm finishes in \(\mathcal{O}(1)\) expected time on every input \(A\). 

  \begin{proof}
    Let \(X\) be the number of iterations until the algorithm picks a zero. Each iteration chooses \(r \in \{1,\dots, n\}\). 

    Then we have 
    \[
      p = \mathbb{P}\left[A[r] = 0\right] = \dfrac{\text{\# of zeros}}{n} = \dfrac{\frac{n}{2}}{n} = \dfrac{1}{2}.
    \]
    Then \(X\) is a geometric random variable with success probability \(p = \frac{1}{2}\). 

    For a geometric random variable, we have
    \[
      \mathbb{E}[X] = \dfrac{1}{p} = \dfrac{1}{\frac{1}{2}} = 2.
    \]
    Since each iteration takes \(\mathcal{O}(1)\) time to pick a random index, we need two such iterations in expectation. Thus, the expected running time is \(2 \times \mathcal{O}(1) = \mathcal{O}(1)\). 
  \end{proof}

  \begin{remark}
    In contrast, any deterministic algorithm must probe at least \(\frac{n}{2}\) integers of \(A\) in the worst case. In other words, any deterministic algorithm must have a worst-case time of \(\Theta(n)\) — provably slower than the above randomized algorithm (in expectation).
  \end{remark}
\end{eg}
